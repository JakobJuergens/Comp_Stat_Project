{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7088b527-0745-4428-90bb-27281e48fe97",
   "metadata": {},
   "source": [
    "Notebook by **Jakob R. Jürgens** - Final project for the course **Computational Statistics** in the summer semester 2021 - Find me at [jakobjuergens.com](https://jakobjuergens.com) <br>\n",
    "# Bias in Feature Selection using Random Forest and possible Improvements\n",
    "---\n",
    "\n",
    "## Introduction <a name=\"introduction\"></a>\n",
    "---\n",
    "\n",
    "* Feature selection using random forests and permutation tests\n",
    "* To address problems of bias towards continuous features or features with many levels\n",
    "\n",
    "## Table of Contents <a name=\"tableofcontents\"></a>\n",
    "---\n",
    "1. [Introduction](#introduction)\n",
    "2. [Table of Contents](#tableofcontents)\n",
    "3. [Preliminary Steps](#preliminaries)\n",
    "4. [Theory](#Theory)\n",
    "    1. [Classification and Regression Trees](#CART)\n",
    "    1. [Random Forests](#RF)\n",
    "    1. [Feature Selection and Importance Measures](#FS_IM)\n",
    "    1. [Selection Biases](#selbias)\n",
    "    1. [Conditional Inference Trees](#CIT)\n",
    "    1. [Conditional Inference Random Forests](#CIRF)\n",
    "    1. [Permutation Approach by Hapfelmeier and Ulm](#hpflmr_ulm)\n",
    "5. [Functions for Simulation](#functions)\n",
    "6. [Simulation Study](#simulation)\n",
    "7. [Application](#application)\n",
    "8. [Bibliography](#bibliography)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de19b2c-ccff-4066-86bf-c2e4ee2ff975",
   "metadata": {},
   "source": [
    "## Preliminary Steps <a name=\"preliminaries\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e542df-9030-46b2-9c3e-fa16d50248eb",
   "metadata": {},
   "source": [
    "Package Installations if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab58a88-1f81-4006-8c5f-eceed561ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"qut\")\n",
    "#install.packages(\"randomForest\")\n",
    "#install.packages(\"party\")\n",
    "#install.packages(\"partykit\")\n",
    "#install.packages(\"parallel\")\n",
    "#install.packages(\"MASS\")\n",
    "#install.packages(\"tidyverse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1eb5b-cbec-485f-80f9-287c22f0f782",
   "metadata": {},
   "source": [
    "Load packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a9e0bc-896f-4107-a6bf-105a2e7cba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages(library(qut)) # for data set \"riboflavin\"\n",
    "suppressMessages(library(randomForest)) # for normal random forests\n",
    "suppressMessages(library(party)) # to allow for conditional inference trees\n",
    "suppressMessages(library(partykit)) # to allow for conditional inference random forests\n",
    "suppressMessages(library(parallel)) # for parallelisation of the simulations\n",
    "suppressMessages(library(MASS)) # for multivariate normal distributions\n",
    "suppressMessages(library(tidyverse)) # for general programming (mostly purrr & ggplot used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ea896-a555-494b-84c2-818530848a52",
   "metadata": {},
   "source": [
    "Load Data set for application part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7102a063-aa45-4d80-91e0-94fe2acf30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(riboflavin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726c290-bbc4-4568-a47b-f13c97d4891b",
   "metadata": {},
   "source": [
    "## Theory <a name=\"theory\"></a>\n",
    "---\n",
    "Before going into the details of the simualtion study conducted in this project, I first inftroduce some theoretical concepts that are necessary to get an informed understanding of the simulation procedures and to interpret the results of said simulations. Additionally I give references to papers that lay the theoretical foundation of these concepts or prove the stated features of the statistical objects. This is also meant to serve as a historical motivation for the simulation study as the methods presented follow an evolution of tree based methods to address a specific class of problems found over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d0859-8ab4-4e95-b455-88120a5b7d2b",
   "metadata": {},
   "source": [
    "### Classification and Regression Trees <a name=\"CART\"></a>\n",
    "---\n",
    "I take the Classification and Regression Tree Algorithm (**CART**) introduced by *Breiman, Friedman, Olshen, and Stone (1984)* as a baseline for the points made in the next paragraphs. So when a bias in the original procedure is addressed, this is to be taken as a statement relating to **CART** and Random Forests built on top of it. Therefore, I am introducing this algorithm first to make the differences clearer when talking about later approaches. I am limiting my description to the regression case of **CART** as regression problems in distinction to classification problems will be the focus of this project. For my presentation of the algorithm I am following Chapter 8 in *James, Witten, Hastie, and Tibshirani (2013)* in my description of the algorithm. <br>\n",
    "The authors split up the process of building a regression tree into the following two steps (quoting from the book)\n",
    "\n",
    "> 1. <em> We divide the predictor space [$X$] — that is, the set of possible values for X<sub>1</sub> , X<sub>2</sub> , . . . , X<sub>p</sub> — into J distinct and non-overlapping regions, R<sub>1</sub> , R<sub>2</sub> , . . . , R<sub>J</sub>. </em>\n",
    "> 2. <em> For every observation that falls into the region R<sub>j</sub>, we make the same prediction, which is simply the mean of the response values for the training observations in R<sub>j</sub>. </em>\n",
    "\n",
    "and elaborate on  both steps in more detail after. <br>\n",
    "Relating to the creating of the distinct non-overlapping regions mentioned in step one choices have to be made in multiple dimensions:\n",
    "* What types of regions are to be considered? \n",
    "* In what sense should a set of regions be optimal i.e. what criterion is used to evaluate a sets performance?\n",
    "* Shall there be a stopping criterion, that prohibits further splitting, even if it would improve the tree's performance according to the chosen criterion?\n",
    "* What method shall be used to find the optimal set?\n",
    "\n",
    "Typical choices for these points are the following:\n",
    "* Only consider regions that are p-dimensional rectangles. This allows for splits to be decided according to the value of one covariate at each node. <br>\n",
    "Other possibilities could be linear combinations of covariates leading to a different tree concept for example.\n",
    "* The criterion to evaluate a set's performance can be chosen quite freely, but a typical choice is the sum of squared residuals (**RSS**).\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{RSS} = \\sum_{j=1}^J \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2\n",
    "\\end{equation}\n",
    "\n",
    "* A typical choice for a stopping criterion is a minimal number of observations in a region. Even if an optimal split would decrease **RSS** for the overall tree, the split is not performed as to not fall under this threshold.\n",
    "* In a setting with a finite amount of observations, it is theoretically possible to evaluate all possible splits (up to identical fitted values) corresponding to regions as described above. <br>\n",
    "This, however, is computationally infeasible. Therefore, an approach called **Recursive Binary Splitting** is taken that follows a greedy, top-down structure.\n",
    "\n",
    "Again quoting from *James, Witten, Hastie, and Tibshirani (2013)*: <br>\n",
    "> <em> The approach is top-down because it begins at the top of the tree [...] and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. </em> <br>\n",
    "> <em> It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. </em> \n",
    "\n",
    "At each step of the tree building process, splits are constructed according to a criterion that evaluates their performance. <br>\n",
    "Assume that **RSS** was chosen as a criterion and that at a node $N_k$ currently represents a p-dimensional rectangle $R_k$. We then construct the next split at this node by minimizing the **RSS** of the full tree over all possible splits at this node. This is equivalent to minimizing the **RSS** on $R_i$ by splitting up $R_k$ into two disjoint p-dimensional rectangles in some in some dimension $j \\in \\{1, \\dots, p\\}$ at a threshold value $s$. In mathematical terms, slightly modifying the definitions from *James, Witten, Hastie, and Tibshirani (2013)*, let \n",
    "\n",
    "\\begin{equation}\n",
    "R_{k,1}(j,s) = R_k \\cap \\{X | X_j < s\\} \\quad R_{k,2}(j,s) = R_k \\cap \\{X | X_j \\geq s\\}\n",
    "\\end{equation}\n",
    "\n",
    "and choose $j$ and $s$ to minimize\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i: x_{i} \\in R_{k,1}(j, s)}\\left(y_{i}-\\hat{y}_{R_{k,1}}\\right)^{2}+\\sum_{i: x_{i} \\in R_{k,2}(j, s)}\\left(y_{i}-\\hat{y}_{R_{k,2}}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}_{R_{k,1}}$ and $\\hat{y}_{R_{k,2}}$ are the means of the dependent variable of the observations in the resulting rectangles. <br>\n",
    "\n",
    "A tree is constructed by recursively applying this procedure to nodes until no further splits can be made without violating the stopping condition (or until no more splits can be made that improve the **RSS**). <br>\n",
    "Given a tree, prediction of the dependent variable given observed covariates is done by checking which p-dimensional rectangle a new observation falls into and using the assigned value of this rectangle.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0088691-5b3f-4504-b2ec-3b6db37d6d80",
   "metadata": {},
   "source": [
    "### Random Forests <a name=\"RF\"></a>\n",
    "---\n",
    "Building up on this rather simple way to construct a tree, there are multiple ways to improve predictive performance among other properties by combining multiple trees. One such method introduced by *Ho (1995)* and extended by *Breiman (2001)* is Random Forest. <br>\n",
    "In contrast to Boosting which uses a combination of weak learners to improve its bias, Random Forest is based on the averaging of strong learners to decrease variance. Analoguous to a simple scalar case concerning for example the estimation of the population mean from a sample, the decrease in variance via averaging depends on the correlation between the data points. In case of Random Forests this relates to the correlation between the predictions of each individual tree. \n",
    "\n",
    "The idea is to construct multiple trees, often deliberately overfitting the data, and to average their results to obtain an overall prediction of the forest. This can for example be used to improve prediction quality but also to improve the procedures qualities for feature selection. <br>To decorrelate the trees, Random Forest deploys two important variations on the original regression tree idea.\n",
    "* Like in bagging each tree is constructed on a bootstrapped sample of the original data. \n",
    "* At each split in a tree, only a subset of dimensions is considered during its creation. A typical choice in the case of p covariates is to randomly choose $m \\approx \\sqrt{p}$ covariates that are considered for the new split. <br>\n",
    "(m = p corresponds to bagging)\n",
    "\n",
    "Mathematically speaking, the construction of each tree happens as follows:\n",
    "1. Construct a bootstrap sample $B_l = \\{b_1^l, \\dots, b_w^l\\}$ of size w from the original data set. <br>\n",
    "(This can also be varied to build trees on subsamples without replacement. For the sake of simplicity, I focus on the bootstrapping approach and later elaborate on cases where subsampling without replacement is used instead.)\n",
    "2. Construct a tree on $B_l$ instead of the original data set as described in the previous section with the following modification:\n",
    "\n",
    "Assume again, that a node $N_k^l$ represents a p-dimensional rectangle $R_k^l$ and that a new split is to be implemented at $N_k^l$. For the sake of simplicity assume that the chosen stopping condition is not violated. <br>\n",
    "Let $M_k^l \\subseteq \\{1, \\dots, p\\}$ with $|M_k^l| = m$ for some $m \\in \\mathcal{N}$. <br>\n",
    "\n",
    "As previously defined, let\n",
    "\n",
    "\\begin{equation}\n",
    "R_{k,1}^l(j,s) = R_k^l \\cap \\{X | X_j < s\\} \\quad R_{k,2}^l(j,s) = R_k^l \\cap \\{X | X_j \\geq s\\}\n",
    "\\end{equation}\n",
    "\n",
    "be the p-dimensional rectangles created by a new split in dimension $j$ at threshold $s$. <br>\n",
    "Now choose $j \\in M_i^l$ and $s$ to minimize\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i: x_{i} \\in R_{k,1}^l(j, s)}\\left(y_{i}-\\hat{y}_{R_{k,1}^l}\\right)^{2}+\\sum_{i: x_{i} \\in R_{k,2}^l(j, s)}\\left(y_{i}-\\hat{y}_{R_{k,2}^l}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}_{R_{k,1}^l}$ and $\\hat{y}_{R_{k,2}^l}$ are the means of the dependent variable of the bootstrap observations in the resulting rectangles. <br>\n",
    "\n",
    "Aggregation of results can happen in a multitude of ways: typical choices in the case of prediction are mean or median of the predicted values from the constructed trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300bc81-b1c5-419e-98f9-8aa789611821",
   "metadata": {},
   "source": [
    "### Feature Selection and Importance Measures <a name=\"FS_IM\"></a>\n",
    "---\n",
    "\n",
    "As this project is mostly concerned with the selection of features with predictive qualities for the dependent variable over raw predictive performance of the overall model, it is important to elaborate on the procedures used to determine a covariate's importance in this regard. Depending of the context the notion of importance for the prediction of a dependent variable can be different. Two reasonable concepts of variable importance employed in different real world scenarios are the following:\n",
    "\n",
    "1. Variables that are informative for the value of the dependent variable themselves or correlated to informative variables should have high variable importance.\n",
    "2. Only variables that are informative for the dependent variable themselves should have high variable importance.\n",
    "\n",
    "As the focus of this project lies on cases with continuous response variables, the importance measures used in the case of regression trees and random forests derived from them are of bigger importance in this context. However, similar measures based on the Gini impurity or Shannon entropy are available for classification problems and much of the following theory carries over. Nevertheless, focus will lie on the case of continuous response variables in the following.\n",
    "\n",
    "#### Variance Reduction\n",
    "\n",
    "A typical importance measure for covariates in regression trees is the sum of reduction in **RSS** that is achieved at notes corresponding to a specific covariate. Recall the explanation of the splitting procedure in regression trees given a few paragraphs above. As the defining factor for the choice of dimension and threshold was the resulting reduction in **RSS**, it seems reasonable to evaluate the importance of a covariate by looking at the reduction in **RSS** in the whole tree attributable to splits in the corresponding dimension. <br>\n",
    "Let $N^j(T)$ be the set of nodes of tree $T$ that implement splits in $X_j$. At each of these nodes there is a corresponding value of **RSS** pre-splitting and post-splitting. Call these values $\\textbf{RSS}_{pre}(N)$ and $\\textbf{RSS}_{post}(N)$ for a generic node $N \\in N^j(T)$. A typical importance measure for the case of the regression variant of **CART** is then given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{VIM}_{RSS}(T, X_j) = \\sum_{N \\in N^j(T)}\\big[ \\textbf{RSS}_{pre}(N) - \\textbf{RSS}_{post}(N) \\big]\n",
    "\\end{equation}\n",
    "\n",
    "For the case of Random Forests, one idea is to carry over this metric and enter consideration via averaging over all trees of the forest. <br>\n",
    "Assuming that $L$ trees $T_{B_1}, \\dots, T_{B_L}$ were constructed based on bootstrap samples $B_1, \\dots, B_L$ as described in the section on random forests, an overall importance measure of covariate $X_j$ could then be implemented as\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{VIM}_{RF}(X_j) = \\frac{1}{L} \\sum_{l = 1}^L \\textbf{VIM}_{RSS}(T_{B_l}, X_j).\n",
    "\\end{equation}\n",
    "\n",
    "This is a rather intuitive criterion. However, it has some flaws and can be improved in some regards using a little more sophistication. As this method will not be in the focus of the later simulations, I limit the extent of this presentation to a short list of ideas as an appropriate discussion is out of the scope of this project. These points can serve as a starting grounds for further literature review for an interested reader.\n",
    "\n",
    "* Instead of using the bootstrapped training data to evaluate the importance of a feature, it is possible and in some cases advisable to base the importance measure on the out of bag (**OOB**) observations that are not part of the bootstrap sample.\n",
    "* The squared sum of residuals is not necessarily optimal in this regard. Prediction accuracy on the **OOB** observations is another reasonable choice.\n",
    "* Weighted averages or the median could be employed instead of the unweighted mean.\n",
    "\n",
    "#### Permutation Accuracy Importance in Random Forests\n",
    "\n",
    "There is also a different kind of importance measure derived from random forests based on a framework of permutation. These methods, though a little more complex than those based on averaging, gained popularity and are more typical in applied work using random forests for feature selection. In my presentation of the basic permutation based importance measures for Random Forest, I am going to follow *Strobl, Boulesteix, Zeileis, and Hothorn (2007)*. The authors give the following rationale for this class of importance measures\n",
    "\n",
    "> By randomly permuting the predictor variable X<sub>j</sub>, its original association with the response Y is broken. When the permuted variable X<sub>j</sub>, together with the remaining unpermuted predictor variables, is used to predict the response, the prediction accuracy [...] decreases substantially, if the original variable X<sub>j</sub> was associated with the response. Thus, a reasonable measure for variable importance is the difference in prediction accuracy before and after permuting X<sub>j</sub>.\n",
    "\n",
    "The example made in the paper relates to a classification problem, where prediction accuracy can be identified by the share of correctly classified observations. In a regression framework with a continuous response variable, an analoguous metric can be defined in terms of the accuracy on the **OOB** observations or the **RSS**. \n",
    "\n",
    "#### In R package randomForest \n",
    "When using the R package *randomForest* (*Liaw and Wiener (2001)*) as I will be in parts of the simulation studies, up to four importance measures are calculated when applying the function *randomForest*. For their definitions  the authors refer to *Breiman (2002)*. The following excerpts are quoted from the manual:\n",
    "\n",
    "Measure 1:\n",
    "> To estimated the importance of the mth variable. In the left out\n",
    "cases for the kth tree, randomly permute all values of the mth\n",
    "variable Put these new covariate values down the tree and get\n",
    "classifications. <br>\n",
    "> Proceed as though computing a new internal error rate. The amount\n",
    "by which this new error exceeds the original test set error is defined\n",
    "as the importance of the mth variable.\n",
    "\n",
    "Measures 2 and 3:\n",
    ">For the nth case in the data, its margin at the end of a run is the\n",
    "proportion of votes for its true class minus the maximum of the\n",
    "proportion of votes for each of the other classes. The 2nd measure\n",
    "of importance of the mth variable is the average lowering of the\n",
    "margin across all cases when the mth variable is randomly permuted\n",
    "as in method 1. <br>\n",
    ">\n",
    ">The third measure is the count of how many margins are lowered\n",
    "minus the number of margins raised.\n",
    "\n",
    "Measure 4:\n",
    "> The splitting criterion used in RF is the gini criterion--also used in\n",
    "CART. At every split one of the mtry variables is used to form the\n",
    "split and there is a resulting decrease in the gini. The sum of all\n",
    "decreases in the forest due to a given variable, normalized by the\n",
    "number of trees, froms (sic) measure 4.\n",
    "\n",
    "\n",
    "\n",
    "#### Remarks\n",
    "When looking at the previously made categorization, both of these measures fall into category 1. and cannot easily be used to separate intrinsically informative variables from those that are only correlated to directly informative variables. \n",
    "\n",
    "#### Conditional Variable Importance for Random Forests\n",
    "\n",
    "Presented in *Strobl, Boulesteix, Kneib, Augustin, and Zeiles (2008)* conditional variable importance measures "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad22823-865a-4a74-bd07-24e9aef05174",
   "metadata": {},
   "source": [
    "### Selection Biases <a name=\"selbias\"></a>\n",
    "---\n",
    "The methods for feature selection described in the previous section have been employed successfully in a plethora of applications. However, theoretical analyses have shown, that there are systematic biases towards specific types of variables exhibited by these methods. *Strobl, Boulesteix, Zeileis, and Hothorn (2007)* have shown, that..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b033d43-0937-4500-9512-03d9d07e99ca",
   "metadata": {},
   "source": [
    "### Conditional Inference Trees <a name=\"CIT\"></a>\n",
    "---\n",
    "Introduced by *Hothorn, Hornik, and Zeileis (2006)*, Conditional Inference Trees (**CIT**) try to address the problem of selection bias of typical random trees based for example on **CART** decribed in the previous paragraph by modifying the recursive binary partitioning procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2767d46-5547-46f3-bb67-af0f357bd5db",
   "metadata": {},
   "source": [
    "### Conditional Inference Random Forests <a name=\"CIRF\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92219a34-ddb4-477c-b7fe-2cc43ddaaf4e",
   "metadata": {},
   "source": [
    "### Permutation Approach by Hapfelmeier and Ulm <a name=\"hpflmr_ulm\"></a>\n",
    "---\n",
    "Developed by *Hapfelmeier and Ulm (2013)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72680def-9d63-486d-a5c6-614d406c12ca",
   "metadata": {},
   "source": [
    "## Functions for Simulation <a name=\"functions\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd717583-87a2-449b-abf1-846a5c61362d",
   "metadata": {},
   "source": [
    "#### permute_dim\n",
    "\n",
    "This function takes an index **i** and a data.frame / tibble **df** and randomly permutes the covariate / the dependent variable corresponding to the given index. <br>\n",
    "As an example **i = 2** corresponds to **X_2**. To permute **Y** set **i = 0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9f5d42a-3fdc-43aa-b0d2-6b02c1577395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i: index of column to permute, if i == 0 permute Y\n",
    "# df: data.frame or tibble with data\n",
    "\n",
    "permute_dim <- function(i = 0, df){\n",
    "    \n",
    "    # check if index is out of bounds\n",
    "    if((i < 0) || (i > dim(df)[2] - 1)){\n",
    "        stop(\"'permute_dim' - index error\")\n",
    "    }\n",
    "    \n",
    "    n_obs <- dim(df)[1]\n",
    "    # permute Y if i == 0\n",
    "    if(i == 0){\n",
    "        df$Y <- sample(df$Y, size = n_obs, replace = FALSE)\n",
    "    } \n",
    "    # Otherwise permute chosen column of X\n",
    "    else{\n",
    "        df[[i+1]] <- sample(df[[i+1]], size = n_obs, replace = FALSE)\n",
    "    }\n",
    "    # return permuted objects for further analysis\n",
    "    return(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7e5b1-16d5-43be-9440-a74bc02829f1",
   "metadata": {},
   "source": [
    "#### parallel_helper\n",
    "This function acts as a helper to the parallelized procedure. Given an index **i** and a data.frame / tibble **df**, it randomly permutes the entry corresponding to the index and creates a conditional inference random forest using the partially permuted data. <br>\n",
    "The conditional inference random forest is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1748fd4-aae0-4bc8-afa1-7f261040c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i: index of column to permute, if i == 0 permute Y\n",
    "# df: data.frame or tibble with data\n",
    "# seed: seed for random number generation\n",
    "\n",
    "parallel_helper <- function(i, df, seed){\n",
    "    \n",
    "    #set seed to ensure reproducibility\n",
    "    set.seed(seed)\n",
    "    \n",
    "    # randomly permute chosen entry\n",
    "    tmp_df <- permute_dim(i, df)\n",
    "    \n",
    "    # return conditional inference random forest\n",
    "    return(cforest(formula = Y ~ ., data = tmp_df))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa0c64-2117-4e73-be31-49f0ad19afc9",
   "metadata": {},
   "source": [
    "#### parallel_permute_forest\n",
    "This function acts as a wrapper to **parallel_helper** and performs non-load-balanced cluster parallelization to speed up the process while still ensuring reproducibility. <br>\n",
    "Its arguments are:\n",
    "* **cl**: a cluster object generated by the parallel package (for UNIX based systems use Forking Cluster, for Windows switch function calls to PSOCK Cluster and ensure that functions are available in the scope of the respective nodes)\n",
    "* **i**: index given to **parallel_helper**\n",
    "* **df**: data.frame / tibble given to **parallel_helper**\n",
    "* **reps**: number of repetitions for **parallel helper**. If provided together with seeds, ensure that the number of seeds is identical to reps.\n",
    "* **df**: vector of integers given to **parallel_helper** as seeds in parallelization. If provided together with reps, ensure that the number of seeds is identical to reps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "756a6401-2e2a-497b-95e7-58b6fa419086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl: cluster object generated by parallel package\n",
    "# i: index of column to permute, if i == 0 permute Y\n",
    "# df: data.frame or tibble with data\n",
    "# reps: number of permutations and corresponding random forests\n",
    "# seeds: seeds for the permutation (set in parallel_helper)\n",
    "\n",
    "parallel_permute_forest <- function(cl, i, df, reps = NULL, seeds = NULL){\n",
    "    \n",
    "    # Check argument structure\n",
    "    if(missing(reps) && missing(seeds)){\n",
    "        stop(\"'parallel_permute_forest' - Insufficient arguments: one of reps or seeds has to be provided\")\n",
    "    } else if(length(seeds) != reps){\n",
    "        stop(\"'parallel_permute_forest' - number of seeds is different from reps\")\n",
    "    } else if(missing(seeds)){\n",
    "        seeds <- sample(1:10e8, size = reps, replace = FALSE)\n",
    "    } else if(missing(reps)){\n",
    "        reps <- length(seeds)\n",
    "    }\n",
    "    \n",
    "    # use non-load-balancing cluster parallelization to speed up the process and ensure reproducibility\n",
    "    cond_inf_rnd_frsts <- clusterApply(x = seeds,\n",
    "                                       fun = function(seed) parallel_helper(i = i, df = df, seed = seed))\n",
    "    \n",
    "    # return list of conditional inference random forests generated in parallel\n",
    "    return(cond_inf_rnd_frsts)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadaa0cd-9aad-4b16-8647-1c78b28007c2",
   "metadata": {},
   "source": [
    "## Simulation Study <a name=\"simulation\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15dd1cf1-c115-4d57-b1f8-a99203695c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Length Class      Mode    \n",
       "nodes    500    -none-     list    \n",
       "data      12    data.frame list    \n",
       "weights  500    -none-     list    \n",
       "fitted     2    data.frame list    \n",
       "terms      3    terms      call    \n",
       "info       2    -none-     list    \n",
       "trafo      1    -none-     function\n",
       "predictf   2    terms      call    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test\n",
    "n = 100\n",
    "p = 10\n",
    "\n",
    "my_mu <- rep(0, times = p)\n",
    "my_Sigma <- diag(rep(1, times = p))\n",
    "X <- cbind(rep(1, times = n), mvrnorm(n = n, mu = my_mu, Sigma = my_Sigma))\n",
    "eps <- rnorm(n = n)\n",
    "\n",
    "beta <- runif(n = p+1, min = -1, max = 1)\n",
    "Y = X %*% beta + eps\n",
    "\n",
    "my_tibble <- cbind(tibble(Y = Y), suppressWarnings(as_tibble(X)))\n",
    "names(my_tibble) <- c('Y', paste0(\"X_\", as.character(1:p)))\n",
    "\n",
    "cond_forest <- cforest(formula = Y ~ ., data = my_tibble)\n",
    "summary(cond_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a5800-e00e-4cfa-810f-a975c10729d5",
   "metadata": {},
   "source": [
    "## Application <a name=\"application\"></a>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88899e63-deec-43d3-82d0-cea8549a03a0",
   "metadata": {},
   "source": [
    "## Bibliography <a name=\"bibliography\"></a>\n",
    "---\n",
    "* Breiman, Leo. “Random Forests.” Machine Learning 45, no. 1 (October 1, 2001): 5–32. https://doi.org/10.1023/A:1010933404324.\n",
    "* Breiman, Leo, Jerome Friedman, Charles J. Stone, and R. A. Olshen. Classification and Regression Trees. Taylor & Francis, 1984.\n",
    "* Hapfelmeier, A., and K. Ulm. “A New Variable Selection Approach Using Random Forests.” Computational Statistics & Data Analysis 60 (April 1, 2013): 50–69. https://doi.org/10.1016/j.csda.2012.09.020.\n",
    "* Ho, Tin Kam. “Random Decision Forests.” In Proceedings of 3rd International Conference on Document Analysis and Recognition, 1:278–82 vol.1, 1995. https://doi.org/10.1109/ICDAR.1995.598994.\n",
    "* Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. “Unbiased Recursive Partitioning: A Conditional Inference Framework.” Journal of Computational and Graphical Statistics 15, no. 3 (September 1, 2006): 651–74. https://doi.org/10.1198/106186006X133933.\n",
    "* Hothorn, Torsten, and Achim Zeileis. “Partykit: A Modular Toolkit for Recursive Partytioning in R.” Journal of Machine Learning Research 16, no. 118 (2015): 3905–9.\n",
    "* Ishwaran, Hemant. “Variable Importance in Binary Regression Trees and Forests.” Electronic Journal of Statistics 1, no. none (January 2007): 519–37. https://doi.org/10.1214/07-EJS039.\n",
    "* James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: With Applications in R. 1st ed. 2013, Corr. 7th printing 2017 edition. New York: Springer, 2013.\n",
    "* Liaw, Andy, and Matthew Wiener. “Classification and Regression by RandomForest.” Forest 23 (November 30, 2001).\n",
    "* Strobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. “Conditional Variable Importance for Random Forests.” BMC Bioinformatics 9 (August 1, 2008): 307. https://doi.org/10.1186/1471-2105-9-307.\n",
    "* Strobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. “Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution.” BMC Bioinformatics 8, no. 1 (January 25, 2007): 25. https://doi.org/10.1186/1471-2105-8-25.\n",
    "\n",
    "\n",
    "* L. Breiman. Manual on setting up, using, and understanding random forests v3.1, 2002. http://oz.berkeley.edu/users/breiman/Using_random_forests_V3.1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d4f61-379b-42ab-9304-32e7c4c62943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
